{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9163231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from transformers import pipeline\n",
    "from ipywidgets import FloatProgress\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78eb6c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e52a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.7581807971000671}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_one = 'Romacnhencko'\n",
    "classifier(text_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c08f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9744058847427368}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_two = 'Anatolyi'\n",
    "classifier(text_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d11a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9895161390304565}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_three = 'FB-21mp'\n",
    "classifier(text_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88feebd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.5971274375915527}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_four = 'IPT'\n",
    "classifier(text_four)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fbdedc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66342083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 12836, 2278, 25311, 2368, 19665, 102], [101, 9617, 3406, 2135, 2072, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([text_one, text_two])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a733d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 7), dtype=int32, numpy=\n",
       "array([[  101, 12836,  2278, 25311,  2368, 19665,   102],\n",
       "       [  101,  9617,  3406,  2135,  2072,   102,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 7), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_padding = tokenizer([text_one, text_two], padding = True, truncation = True, max_length = 256, return_tensors=\"tf\")\n",
    "inputs_with_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37eebc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[ 0.69444466, -0.44829053],\n",
       "        [-1.778941  ,  1.8605208 ]], dtype=float32)>,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(inputs_with_padding)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99923d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.7581815 , 0.24181853],\n",
       "       [0.0255942 , 0.9744058 ]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = tf.nn.softmax(outputs[0], axis=-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e168ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 12836, 2278, 25311, 2368, 19665, 102], [101, 9617, 3406, 2135, 2072, 102], [101, 1042, 2497, 1011, 2538, 8737, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([text_one, text_two, text_three])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f515fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(3, 7), dtype=int32, numpy=\n",
       "array([[  101, 12836,  2278, 25311,  2368, 19665,   102],\n",
       "       [  101,  9617,  3406,  2135,  2072,   102,     0],\n",
       "       [  101,  1042,  2497,  1011,  2538,  8737,   102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(3, 7), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_padding = tokenizer([text_one, text_two, text_three], padding = True, truncation = True, max_length = 256, return_tensors=\"tf\")\n",
    "inputs_with_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40dd0e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[ 0.6944459, -0.4482916],\n",
       "        [-1.7789414,  1.8605213],\n",
       "        [ 2.4833722, -2.0640042]], dtype=float32)>,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(inputs_with_padding)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1774d7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[0.7581819 , 0.24181813],\n",
       "       [0.02559419, 0.9744058 ],\n",
       "       [0.98951614, 0.01048389]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = tf.nn.softmax(outputs[0], axis=-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eae1a715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 12836, 2278, 25311, 2368, 19665, 102], [101, 9617, 3406, 2135, 2072, 102], [101, 1042, 2497, 1011, 2538, 8737, 102], [101, 12997, 2102, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([text_one, text_two, text_three, text_four])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d488971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(4, 7), dtype=int32, numpy=\n",
       "array([[  101, 12836,  2278, 25311,  2368, 19665,   102],\n",
       "       [  101,  9617,  3406,  2135,  2072,   102,     0],\n",
       "       [  101,  1042,  2497,  1011,  2538,  8737,   102],\n",
       "       [  101, 12997,  2102,   102,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(4, 7), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_padding = tokenizer([text_one, text_two, text_three, text_four], padding = True, truncation = True, max_length = 256, return_tensors=\"tf\")\n",
    "inputs_with_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ecf9179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       " array([[ 0.69444466, -0.4482905 ],\n",
       "        [-1.7789401 ,  1.8605196 ],\n",
       "        [ 2.4833715 , -2.0640037 ],\n",
       "        [ 0.3297063 , -0.06381004]], dtype=float32)>,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(inputs_with_padding)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "627f2bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/kali/anaconda3/envs/teststackoverflow/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1321: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'Yes', 'end': 2, 'score': 0.5990897417068481, 'start': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_qusetion_answering = pipeline('question-answering')\n",
    "nlp_qusetion_answering(context='Yes', question='Will Ukraine win?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
